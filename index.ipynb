{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Extensions to Linear Models - Lab"]},{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","In this lab, you'll practice many concepts you have learned so far, from adding interactions and polynomials to your model to regularization!"]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n","\n","You will be able to:\n","\n","- Build a linear regression model with interactions and polynomial features \n","- Use feature selection to obtain the optimal subset of features in a dataset"]},{"cell_type":"markdown","metadata":{},"source":["## Let's Get Started!\n","\n","Below we import all the necessary packages for this lab."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Run this cell without changes\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings('ignore')\n","from itertools import combinations\n","\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LinearRegression, Lasso\n","from sklearn.metrics import mean_squared_error\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, PolynomialFeatures"]},{"cell_type":"markdown","metadata":{},"source":["Load the data."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Run this cell without changes\n","\n","# Load data from CSV\n","df = pd.read_csv(\"ames.csv\")\n","# Subset columns\n","df = df[['LotArea', 'OverallQual', 'OverallCond', 'TotalBsmtSF',\n","         '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'TotRmsAbvGrd',\n","         'GarageArea', 'Fireplaces', 'SalePrice']]\n","\n","# Split the data into X and y\n","y = df['SalePrice']\n","X = df.drop(columns='SalePrice')\n","\n","# Split into train, test, and validation sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=0)"]},{"cell_type":"markdown","metadata":{},"source":["## Build a Baseline Housing Data Model"]},{"cell_type":"markdown","metadata":{},"source":["Above, we imported the Ames housing data and grabbed a subset of the data to use in this analysis.\n","\n","Next steps:\n","\n","- Scale all the predictors using `StandardScaler`, then convert these scaled features back into DataFrame objects\n","- Build a baseline `LinearRegression` model using *scaled variables* as predictors and use the $R^2$ score to evaluate the model "]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Initialize the StandardScaler\n","scaler = StandardScaler()\n","\n","# Fit the scaler on the training data and transform both training and validation data\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_val_scaled = scaler.transform(X_val)\n","\n","# Convert the scaled arrays back to DataFrames\n","X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n","X_val_scaled = pd.DataFrame(X_val_scaled, columns=X.columns)\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Baseline R-squared score on training data: 0.7868344817421309\n"]}],"source":["# Create a LinearRegression model\n","baseline_model = LinearRegression()\n","\n","# Fit the model on the scaled training data\n","baseline_model.fit(X_train_scaled, y_train)\n","\n","# Calculate a baseline r-squared score on the training data\n","baseline_r2 = baseline_model.score(X_train_scaled, y_train)\n","print(f\"Baseline R-squared score on training data: {baseline_r2}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Add Interactions\n","\n","Instead of adding all possible interaction terms, let's try a custom technique. We are only going to add the interaction terms that increase the $R^2$ score as much as possible. Specifically we are going to look for the 7 interaction terms that each cause the most increase in the coefficient of determination.\n","\n","### Find the Best Interactions\n","\n","Look at all the possible combinations of variables for interactions by adding interactions one by one to the baseline model. Create a data structure that stores the pair of columns used as well as the $R^2$ score for each combination.\n","\n","***Hint:*** We have imported the `combinations` function from `itertools` for you ([documentation here](https://docs.python.org/3/library/itertools.html#itertools.combinations)). Try applying this to the columns of `X_train` to find all of the possible pairs.\n","\n","Print the 7 interactions that result in the highest $R^2$ scores."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Interaction: LotArea_1stFlrSF, R-squared: 0.7211105666140573\n","Interaction: LotArea_TotalBsmtSF, R-squared: 0.7071649207050115\n","Interaction: LotArea_GrLivArea, R-squared: 0.6690980823779022\n","Interaction: LotArea_Fireplaces, R-squared: 0.6529699515652587\n","Interaction: 2ndFlrSF_TotRmsAbvGrd, R-squared: 0.6472994890405193\n","Interaction: OverallCond_TotalBsmtSF, R-squared: 0.6429019879233768\n","Interaction: OverallQual_2ndFlrSF, R-squared: 0.6422324294284368\n"]}],"source":["# Your code here\n","\n","# Set up data structure\n","interaction_scores = []\n","\n","# Find combinations of columns and loop over them\n","for col1, col2 in combinations(X_train.columns, 2):\n","    # Make copies of X_train and X_val\n","    X_train_copy = X_train_scaled.copy()\n","    X_val_copy = X_val_scaled.copy()\n","    \n","    # Add interaction term to data\n","    interaction_term = X_train_copy[col1] * X_train_copy[col2]\n","    interaction_term_val = X_val_copy[col1] * X_val_copy[col2]\n","    interaction_name = f\"{col1}_{col2}\"\n","    X_train_copy[interaction_name] = interaction_term\n","    X_val_copy[interaction_name] = interaction_term_val\n","    \n","    # Find r-squared score (fit on training data, evaluate on validation data)\n","    model = LinearRegression()\n","    model.fit(X_train_copy, y_train)\n","    r2_score = model.score(X_val_copy, y_val)\n","    \n","    # Append to data structure\n","    interaction_scores.append((interaction_name, r2_score))\n","\n","# Sort and subset the data structure to find the top 7\n","interaction_scores.sort(key=lambda x: x[1], reverse=True)\n","top_7_interactions = interaction_scores[:7]\n","\n","# Print the top 7 interactions\n","for interaction, score in top_7_interactions:\n","    print(f\"Interaction: {interaction}, R-squared: {score}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Add the Best Interactions\n","\n","Write code to include the 7 most important interactions in `X_train` and `X_val` by adding 7 columns. Use the naming convention `\"col1_col2\"`, where `col1` and `col2` are the two columns in the interaction."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# Loop over top 7 interactions\n","for interaction, score in top_7_interactions:\n","    # Extract column names from data structure\n","    col1, col2 = interaction.split('_')\n","    \n","    # Construct new column name\n","    interaction_name = f\"{col1}_{col2}\"\n","    \n","    # Add new column to X_train and X_val\n","    X_train_scaled[interaction_name] = X_train_scaled[col1] * X_train_scaled[col2]\n","    X_val_scaled[interaction_name] = X_val_scaled[col1] * X_val_scaled[col2]\n"]},{"cell_type":"markdown","metadata":{},"source":["## Add Polynomials\n","\n","Now let's repeat that process for adding polynomial terms.\n","\n","### Find the Best Polynomials\n","\n","Try polynomials of degrees 2, 3, and 4 for each variable, in a similar way you did for interactions (by looking at your baseline model and seeing how $R^2$ increases). Do understand that when going for a polynomial of degree 4 with `PolynomialFeatures`, the particular column is raised to the power of 2 and 3 as well in other terms.\n","\n","We only want to include \"pure\" polynomials, so make sure no interactions are included.\n","\n","Once again you should make a data structure that contains the values you have tested. We recommend a list of tuples of the form:\n","\n","`(col_name, degree, R2)`, so eg. `('OverallQual', 2, 0.781)` "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Column: GrLivArea, Degree: 3, R-squared: 0.8283345026130584\n","Column: OverallQual, Degree: 3, R-squared: 0.8068023565969227\n","Column: OverallQual, Degree: 4, R-squared: 0.8033455378866836\n","Column: OverallQual, Degree: 2, R-squared: 0.802583337359408\n","Column: 2ndFlrSF, Degree: 3, R-squared: 0.7767514355074356\n","Column: 2ndFlrSF, Degree: 4, R-squared: 0.7696909328107031\n","Column: LotArea, Degree: 3, R-squared: 0.7647014750792784\n"]}],"source":["# Set up data structure\n","polynomial_scores = []\n","\n","# Loop over all columns\n","for col in X_train.columns:\n","    # Loop over degrees 2, 3, 4\n","    for degree in [2, 3, 4]:\n","        # Make a copy of X_train and X_val\n","        X_train_copy = X_train_scaled.copy()\n","        X_val_copy = X_val_scaled.copy()\n","        \n","        # Instantiate PolynomialFeatures with relevant degree\n","        poly = PolynomialFeatures(degree=degree, include_bias=False)\n","        \n","        # Fit polynomial to column and transform column\n","        poly_train = poly.fit_transform(X_train[[col]])\n","        poly_val = poly.transform(X_val[[col]])\n","        \n","        # Convert the result to a DataFrame\n","        poly_train_df = pd.DataFrame(poly_train, columns=[f\"{col}^{i}\" for i in range(1, degree+1)])\n","        poly_val_df = pd.DataFrame(poly_val, columns=[f\"{col}^{i}\" for i in range(1, degree+1)])\n","        \n","        # Add polynomial to data\n","        X_train_copy = pd.concat([X_train_copy.drop(columns=[col]), poly_train_df], axis=1)\n","        X_val_copy = pd.concat([X_val_copy.drop(columns=[col]), poly_val_df], axis=1)\n","        \n","        # Find r-squared score on validation\n","        model = LinearRegression()\n","        model.fit(X_train_copy, y_train)\n","        r2_score = model.score(X_val_copy, y_val)\n","        \n","        # Append to data structure\n","        polynomial_scores.append((col, degree, r2_score))\n","\n","# Sort and subset the data structure to find the top 7\n","polynomial_scores.sort(key=lambda x: x[2], reverse=True)\n","top_7_polynomials = polynomial_scores[:7]\n","\n","# Print the top 7 polynomials\n","for col, degree, score in top_7_polynomials:\n","    print(f\"Column: {col}, Degree: {degree}, R-squared: {score}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["### Add the Best Polynomials\n","\n","If there are duplicate column values in the results above, don't add multiple of them to the model, to avoid creating duplicate columns. (For example, if column `A` appeared in your list as both a 2nd and 3rd degree polynomial, adding both would result in `A` squared being added to the features twice.) Just add in the polynomial that results in the highest R-Squared."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Your code here\n","\n","# Filter out duplicates\n","unique_polynomials = {}\n","for col, degree, score in top_7_polynomials:\n","    if col not in unique_polynomials or unique_polynomials[col][1] < score:\n","        unique_polynomials[col] = (degree, score)\n","\n","# Loop over remaining results\n","for col, (degree, score) in unique_polynomials.items():\n","    # Create polynomial terms\n","    poly = PolynomialFeatures(degree=degree, include_bias=False)\n","    poly_train = poly.fit_transform(X_train_scaled[[col]])\n","    poly_val = poly.transform(X_val_scaled[[col]])\n","    \n","    # Convert the result to a DataFrame\n","    poly_train_df = pd.DataFrame(poly_train, columns=[f\"{col}^{i}\" for i in range(1, degree+1)])\n","    poly_val_df = pd.DataFrame(poly_val, columns=[f\"{col}^{i}\" for i in range(1, degree+1)])\n","    \n","    # Concat new polynomials to X_train and X_val\n","    X_train_scaled = pd.concat([X_train_scaled.drop(columns=[col]), poly_train_df], axis=1)\n","    X_val_scaled = pd.concat([X_val_scaled.drop(columns=[col]), poly_val_df], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["Check out your final data set and make sure that your interaction terms as well as your polynomial terms are included."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Columns in X_train_scaled:\n","Index(['OverallCond', 'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'GarageArea',\n","       'Fireplaces', 'LotArea_1stFlrSF', 'LotArea_TotalBsmtSF',\n","       'LotArea_GrLivArea', 'LotArea_Fireplaces', '2ndFlrSF_TotRmsAbvGrd',\n","       'OverallCond_TotalBsmtSF', 'OverallQual_2ndFlrSF', 'GrLivArea^1',\n","       'GrLivArea^2', 'GrLivArea^3', 'OverallQual^1', 'OverallQual^2',\n","       'OverallQual^3', '2ndFlrSF^1', '2ndFlrSF^2', '2ndFlrSF^3', 'LotArea^1',\n","       'LotArea^2', 'LotArea^3'],\n","      dtype='object')\n","\n","Columns in X_val_scaled:\n","Index(['OverallCond', 'TotalBsmtSF', '1stFlrSF', 'TotRmsAbvGrd', 'GarageArea',\n","       'Fireplaces', 'LotArea_1stFlrSF', 'LotArea_TotalBsmtSF',\n","       'LotArea_GrLivArea', 'LotArea_Fireplaces', '2ndFlrSF_TotRmsAbvGrd',\n","       'OverallCond_TotalBsmtSF', 'OverallQual_2ndFlrSF', 'GrLivArea^1',\n","       'GrLivArea^2', 'GrLivArea^3', 'OverallQual^1', 'OverallQual^2',\n","       'OverallQual^3', '2ndFlrSF^1', '2ndFlrSF^2', '2ndFlrSF^3', 'LotArea^1',\n","       'LotArea^2', 'LotArea^3'],\n","      dtype='object')\n"]}],"source":["# Print the columns of the final datasets\n","print(\"Columns in X_train_scaled:\")\n","print(X_train_scaled.columns)\n","\n","print(\"\\nColumns in X_val_scaled:\")\n","print(X_val_scaled.columns)"]},{"cell_type":"markdown","metadata":{},"source":["## Full Model R-Squared"]},{"cell_type":"markdown","metadata":{},"source":["Check out the $R^2$ of the full model with your interaction and polynomial terms added. Print this value for both the train and validation data."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Full model R-squared score on training data: 0.8514716747836066\n","Full model R-squared score on validation data: 0.795831492875674\n"]}],"source":["# Fit the model on the full training data with interactions and polynomial terms\n","full_model = LinearRegression()\n","full_model.fit(X_train_scaled, y_train)\n","\n","# Calculate the R-squared score on the training data\n","train_r2 = full_model.score(X_train_scaled, y_train)\n","print(f\"Full model R-squared score on training data: {train_r2}\")\n","\n","# Calculate the R-squared score on the validation data\n","val_r2 = full_model.score(X_val_scaled, y_val)\n","print(f\"Full model R-squared score on validation data: {val_r2}\")"]},{"cell_type":"markdown","metadata":{},"source":["It looks like we may be overfitting some now. Let's try some feature selection techniques."]},{"cell_type":"markdown","metadata":{},"source":["## Feature Selection\n","\n","First, test out `RFE` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html)) with several different `n_features_to_select` values. For each value, print out the train and validation $R^2$ score and how many features remain."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["n_features_to_select: 5\n","Train R-squared: 0.776039994126505\n","Validation R-squared: 0.6352981725272362\n","Number of features selected: 5\n","------------------------------\n","n_features_to_select: 10\n","Train R-squared: 0.8235682746440406\n","Validation R-squared: 0.7699939539204789\n","Number of features selected: 10\n","------------------------------\n","n_features_to_select: 15\n","Train R-squared: 0.8357048475147715\n","Validation R-squared: 0.8301279241242279\n","Number of features selected: 15\n","------------------------------\n","n_features_to_select: 20\n","Train R-squared: 0.8500601361669062\n","Validation R-squared: 0.7904975647411846\n","Number of features selected: 20\n","------------------------------\n","n_features_to_select: 25\n","Train R-squared: 0.8514716747836066\n","Validation R-squared: 0.795831492875674\n","Number of features selected: 25\n","------------------------------\n"]}],"source":["# Define a range of n_features_to_select values\n","n_features_range = [5, 10, 15, 20, 25]\n","\n","# Loop over the range of n_features_to_select values\n","for n_features in n_features_range:\n","    # Initialize RFE with the LinearRegression model and the current n_features_to_select\n","    rfe = RFE(estimator=LinearRegression(), n_features_to_select=n_features)\n","    \n","    # Fit RFE on the scaled training data\n","    rfe.fit(X_train_scaled, y_train)\n","    \n","    # Calculate the R-squared score on the training data\n","    train_r2_rfe = rfe.score(X_train_scaled, y_train)\n","    \n","    # Calculate the R-squared score on the validation data\n","    val_r2_rfe = rfe.score(X_val_scaled, y_val)\n","    \n","    # Print the results\n","    print(f\"n_features_to_select: {n_features}\")\n","    print(f\"Train R-squared: {train_r2_rfe}\")\n","    print(f\"Validation R-squared: {val_r2_rfe}\")\n","    print(f\"Number of features selected: {n_features}\")\n","    print(\"-\" * 30)\n"]},{"cell_type":"markdown","metadata":{},"source":["Now test out `Lasso` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)) with several different `alpha` values."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Alpha: 0.01\n","Train R-squared: 0.8514716704440692\n","Validation R-squared: 0.7958035206136165\n","Number of features used: 25\n","------------------------------\n","Alpha: 0.1\n","Train R-squared: 0.8514716692171405\n","Validation R-squared: 0.7958024299368864\n","Number of features used: 25\n","------------------------------\n","Alpha: 1\n","Train R-squared: 0.8514716337254029\n","Validation R-squared: 0.7957915028842499\n","Number of features used: 25\n","------------------------------\n","Alpha: 10\n","Train R-squared: 0.8514689564926174\n","Validation R-squared: 0.7956802495054591\n","Number of features used: 25\n","------------------------------\n","Alpha: 100\n","Train R-squared: 0.8512212885336649\n","Validation R-squared: 0.7964934924025951\n","Number of features used: 24\n","------------------------------\n"]}],"source":["# Define a range of alpha values\n","alpha_range = [0.01, 0.1, 1, 10, 100]\n","\n","# Loop over the range of alpha values\n","for alpha in alpha_range:\n","    # Initialize Lasso with the current alpha value\n","    lasso = Lasso(alpha=alpha)\n","    \n","    # Fit Lasso on the scaled training data\n","    lasso.fit(X_train_scaled, y_train)\n","    \n","    # Calculate the R-squared score on the training data\n","    train_r2_lasso = lasso.score(X_train_scaled, y_train)\n","    \n","    # Calculate the R-squared score on the validation data\n","    val_r2_lasso = lasso.score(X_val_scaled, y_val)\n","    \n","    # Print the results\n","    print(f\"Alpha: {alpha}\")\n","    print(f\"Train R-squared: {train_r2_lasso}\")\n","    print(f\"Validation R-squared: {val_r2_lasso}\")\n","    print(f\"Number of features used: {np.sum(lasso.coef_ != 0)}\")\n","    print(\"-\" * 30)\n"]},{"cell_type":"markdown","metadata":{},"source":["Compare the results. Which features would you choose to use?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Your written answer here"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"data":{"text/plain":["\"\\nFor RFE the model with the best validation R-Squared was using 20 features\\n\\nFor Lasso the model with the best validation R-Squared was using an alpha of 10000\\n\\nThe Lasso result was a bit better so let's go with that and the 12 features\\nthat it selected\\n\""]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","For RFE the model with the best validation R-Squared was using 20 features\n","\n","For Lasso the model with the best validation R-Squared was using an alpha of 10000\n","\n","The Lasso result was a bit better so let's go with that and the 12 features\n","that it selected\n","\"\"\""]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate Final Model on Test Data\n","\n","### Data Preparation\n","\n","At the start of this lab, we created `X_test` and `y_test`. Prepare `X_test` the same way that `X_train` and `X_val` have been prepared. This includes scaling, adding interactions, and adding polynomial terms."]},{"cell_type":"code","execution_count":null,"metadata":{"vscode":{"languageId":"julia"}},"outputs":[],"source":["# Scale the test data\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Convert the scaled array back to DataFrame\n","X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n","\n","# Add the top 7 interaction terms to the test data\n","for interaction, score in top_7_interactions:\n","    col1, col2 = interaction.split('_')\n","    interaction_name = f\"{col1}_{col2}\"\n","    X_test_scaled[interaction_name] = X_test_scaled[col1] * X_test_scaled[col2]\n","\n","# Add the top polynomial terms to the test data\n","for col, (degree, score) in unique_polynomials.items():\n","    poly = PolynomialFeatures(degree=degree, include_bias=False)\n","    poly_test = poly.fit_transform(X_test_scaled[[col]])\n","    poly_test_df = pd.DataFrame(poly_test, columns=[f\"{col}^{i}\" for i in range(1, degree+1)])\n","    X_test_scaled = pd.concat([X_test_scaled.drop(columns=[col]), poly_test_df], axis=1)\n","\n","# Print the columns of the final test dataset to verify\n","print(\"Columns in X_test_scaled:\")\n","print(X_test_scaled.columns)"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluation"]},{"cell_type":"markdown","metadata":{},"source":["Using either `RFE` or `Lasso`, fit a model on the complete train + validation set, then find R-Squared and MSE values for the test set."]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test R-squared score: 0.8516556180721855\n","Test Mean Squared Error: 1039198241.5826004\n"]}],"source":["from sklearn.metrics import mean_squared_error\n","\n","# Scale the test data\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Convert the scaled array back to DataFrame\n","X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n","\n","# Add the top 7 interaction terms to the test data\n","for interaction, score in top_7_interactions:\n","    col1, col2 = interaction.split('_')\n","    interaction_name = f\"{col1}_{col2}\"\n","    X_test_scaled[interaction_name] = X_test_scaled[col1] * X_test_scaled[col2]\n","\n","# Add the top polynomial terms to the test data\n","for col, (degree, score) in unique_polynomials.items():\n","    poly = PolynomialFeatures(degree=degree, include_bias=False)\n","    poly_test = poly.fit_transform(X_test_scaled[[col]])\n","    poly_test_df = pd.DataFrame(poly_test, columns=[f\"{col}^{i}\" for i in range(1, degree+1)])\n","    X_test_scaled = pd.concat([X_test_scaled.drop(columns=[col]), poly_test_df], axis=1)\n","\n","# Combine train and validation sets\n","X_train_val_scaled = pd.concat([X_train_scaled, X_val_scaled])\n","y_train_val = pd.concat([y_train, y_val])\n","\n","# Fit the Lasso model on the combined train + validation set\n","lasso_final = Lasso(alpha=100)\n","lasso_final.fit(X_train_val_scaled, y_train_val)\n","\n","# Calculate the R-squared score on the test data\n","test_r2 = lasso_final.score(X_test_scaled, y_test)\n","print(f\"Test R-squared score: {test_r2}\")\n","\n","# Calculate the Mean Squared Error on the test data\n","test_mse = mean_squared_error(y_test, lasso_final.predict(X_test_scaled))\n","print(f\"Test Mean Squared Error: {test_mse}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Level Up Ideas (Optional)\n","\n","### Create a Lasso Path\n","\n","From this section, you know that when using `Lasso`, more parameters shrink to zero as your regularization parameter goes up. In scikit-learn there is a function `lasso_path()` which visualizes the shrinkage of the coefficients while $alpha$ changes. Try this out yourself!\n","\n","https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py\n","\n","### AIC and BIC for Subset Selection\n","\n","This notebook shows how you can use AIC and BIC purely for feature selection. Try this code out on our Ames housing data!\n","\n","https://xavierbourretsicotte.github.io/subset_selection.html"]},{"cell_type":"markdown","metadata":{},"source":["## Summary"]},{"cell_type":"markdown","metadata":{},"source":["Congratulations! You now know how to apply concepts of bias-variance tradeoff using extensions to linear models and feature selection."]}],"metadata":{"kernelspec":{"display_name":"Cohort_Env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":4}
